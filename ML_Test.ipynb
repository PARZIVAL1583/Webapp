{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d86f8dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter Case Study  Cybercriminals gained access to victims' internet banking credentials, fraudulently transferred funds from their accounts, and induced the delivery of property to unauthorized recipients. The offenders were charged with cheating under Section 420.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import bz2\n",
    "sfile1 = bz2.BZ2File('All Model', 'r')\n",
    "models=pickle.load(sfile1)\n",
    "sfile2 = bz2.BZ2File('All Vector', 'r')\n",
    "vectorizer=pickle.load(sfile2)\n",
    "cas=input(\"Enter Case Study\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6ca56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df=pd.DataFrame({\"case study\":[cas]})\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cb5c08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk \n",
    "def remove_stopwords(text):\n",
    "    stopwords=nltk.corpus.stopwords.words('english')\n",
    "    clean_text=' '.join([word for word in text.split() if word not in stopwords])\n",
    "    return clean_text\n",
    "\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "def cleanup_data(df):\n",
    "    # remove handle\n",
    "    df['clean'] = df[\"case study\"].str.replace(\"@\", \"\") \n",
    "    # remove links\n",
    "    df['clean'] = df['clean'].str.replace(r\"http\\S+\", \"\") \n",
    "    # remove punctuations and special characters\n",
    "    df['clean'] = df['clean'].str.replace(\"[^a-zA-Z]\", \" \") \n",
    "    # remove stop words\n",
    "    df['clean'] = df['clean'].apply(lambda text : remove_stopwords(text.lower()))\n",
    "    # split text and tokenize\n",
    "    df['clean'] = df['clean'].apply(lambda x: x.split())\n",
    "    # let's apply stemmer\n",
    "    stemmer = PorterStemmer()\n",
    "    df['clean'] = df['clean'].apply(lambda x: [stemmer.stem(i) for i in x])\n",
    "    # stitch back words\n",
    "    df['clean'] = df['clean'].apply(lambda x: ' '.join([w for w in x]))\n",
    "    # remove small words\n",
    "    df['clean'] = df['clean'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\n",
    "cleanup_data(df)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48ec816d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"clean\"][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5daeacd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature=vectorizer.transform([df[\"clean\"][0]])\n",
    "feature.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31e82055",
   "metadata": {},
   "outputs": [],
   "source": [
    "names = [\"K-Nearest Neighbors\", \"Liner SVM\",\n",
    "         \"Decision Tree\", \"Random Forest\",\n",
    "         \"ExtraTreesClassifier\"]\n",
    "for i in range(0,len(names)):\n",
    "    print(names[i])\n",
    "    print(models[i].predict(feature))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
